{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With Text Embeddings\n",
    "\n",
    "## A Complete Guide to Understanding and Using Embeddings in AI\n",
    "\n",
    "In this tutorial, you'll learn:\n",
    "- What text embeddings are and why they matter\n",
    "- How to generate embeddings using SentenceTransformer\n",
    "- How to measure similarity between texts\n",
    "- How to build a practical semantic search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install sentence-transformers scikit-learn numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For nice visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the embedding model (downloads on first run)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö What Are Text Embeddings?\n",
    "\n",
    "**Text embeddings** are numerical representations of text that capture semantic meaning. \n",
    "\n",
    "**Key insight**: Similar meanings = Similar number patterns!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function to Get Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model_instance=model):\n",
    "    \"\"\"\n",
    "    Get embedding vector for a given text using SentenceTransformer.\n",
    "    \n",
    "    Args:\n",
    "        text: String to embed\n",
    "        model_instance: SentenceTransformer model instance\n",
    "    \n",
    "    Returns:\n",
    "        List of floats (the embedding vector)\n",
    "    \"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")  # Clean text\n",
    "    embedding = model_instance.encode(text, convert_to_tensor=False)\n",
    "    return embedding.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Example : Your First Embedding - A Single Word\n",
    "\n",
    "Let's convert the word \"rocket\" into an embedding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"rocket\"\n",
    "embedding = get_embedding(word)\n",
    "\n",
    "print(f\"Word: '{word}'\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"\\nFirst 15 values of the embedding:\")\n",
    "print(embedding[:15])\n",
    "print(\"\\nüìù Note: all-mpnet-base-v2 uses 768 dimensions for better semantic quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Embedding\n",
    "\n",
    "Let's look at what these numbers look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize part of the embedding\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.plot(embedding[:100], marker='o', markersize=2, linewidth=0.5)\n",
    "plt.title(f\"First 100 Dimensions of '{word}' Embedding\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Dimension\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Example : Embedding Complete Sentences\n",
    "\n",
    "Embeddings work for entire sentences too! The model captures the FULL meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Artificial intelligence is revolutionizing healthcare\"\n",
    "embedding = get_embedding(sentence)\n",
    "\n",
    "print(f\"Sentence: '{sentence}'\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"\\nFirst 15 values:\")\n",
    "print(embedding[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Understanding Cosine Similarity\n",
    "\n",
    "**Cosine similarity** measures how \"close\" two embeddings are:\n",
    "\n",
    "\n",
    "### Visual Analogy:\n",
    "```\n",
    "Vector A ‚Üí \n",
    "          \\  Small angle = High similarity (close to 1.0)\n",
    "           ‚Üí Vector B\n",
    "\n",
    "Vector A ‚Üë\n",
    "        |\n",
    "        |  Large angle = Low similarity (close to 0.0)\n",
    "        |________‚Üí Vector C\n",
    "```\n",
    "\n",
    "### The Math:\n",
    "```\n",
    "similarity = cos(angle) = (A ¬∑ B) / (||A|| √ó ||B||)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Calculate Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
    "    emb1 = np.array(get_embedding(text1)).reshape(1, -1)\n",
    "    emb2 = np.array(get_embedding(text2)).reshape(1, -1)\n",
    "    return cosine_similarity(emb1, emb2)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Example : Comparing Similar vs Different Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_a = \"I adopted a golden retriever puppy yesterday\"\n",
    "sentence_b = \"My new dog is very playful and energetic\"\n",
    "sentence_c = \"The stock market crashed last Monday\"\n",
    "\n",
    "sim_ab = calculate_similarity(sentence_a, sentence_b)\n",
    "sim_ac = calculate_similarity(sentence_a, sentence_c)\n",
    "sim_bc = calculate_similarity(sentence_b, sentence_c)\n",
    "\n",
    "print(\"Sentences:\")\n",
    "print(f\"A: '{sentence_a}'\")\n",
    "print(f\"B: '{sentence_b}'\")\n",
    "print(f\"C: '{sentence_c}'\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Similarity A ‚Üî B: {sim_ab:.4f}  (both about dogs)\")\n",
    "print(f\"Similarity A ‚Üî C: {sim_ac:.4f}  (dogs vs stocks - unrelated)\")\n",
    "print(f\"Similarity B ‚Üî C: {sim_bc:.4f}  (dogs vs stocks - unrelated)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example : Similarity Heatmap\n",
    "\n",
    "Let's visualize similarities between multiple sentences at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Machine learning models need training data\",\n",
    "    \"Neural networks require lots of examples\",\n",
    "    \"I'm planning a vacation to Hawaii\",\n",
    "    \"My trip to the beach is next month\",\n",
    "    \"Climate change affects ocean temperatures\"\n",
    "]\n",
    "\n",
    "# Calculate all pairwise similarities\n",
    "n = len(sentences)\n",
    "similarity_matrix = np.zeros((n, n))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        if i == j:\n",
    "            similarity_matrix[i][j] = 1.0\n",
    "        elif i < j:\n",
    "            sim = calculate_similarity(sentences[i], sentences[j])\n",
    "            similarity_matrix[i][j] = sim\n",
    "            similarity_matrix[j][i] = sim  # Symmetric\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(11, 9))\n",
    "sns.heatmap(\n",
    "    similarity_matrix,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='YlOrRd',\n",
    "    xticklabels=[f\"S{i+1}\" for i in range(n)],\n",
    "    yticklabels=[f\"S{i+1}\" for i in range(n)],\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Similarity Score'},\n",
    "    square=True\n",
    ")\n",
    "plt.title(\"Sentence Similarity Heatmap\\n\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "\n",
    "print(\"\\nüìã Sentence Reference:\")\n",
    "for i, sent in enumerate(sentences):\n",
    "    print(f\"S{i+1}: {sent}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice:\")\n",
    "print(\"- S1 & S2 are similar (both about ML)\")\n",
    "print(\"- S3 & S4 are similar (both about travel)\")\n",
    "print(\"- S5 is different from all others\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîé Real-World Application: Semantic Search Engine\n",
    "\n",
    "Let's build a simple search engine that finds documents by **MEANING**, not just keywords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our document database\n",
    "documents = [\n",
    "    \"How to train a neural network for image classification\",\n",
    "    \"Best beaches to visit in summer for families\",\n",
    "    \"Understanding backpropagation in deep learning\",\n",
    "    \"Top hiking trails in the Rocky Mountains\",\n",
    "    \"Introduction to convolutional neural networks\",\n",
    "    \"Planning a road trip across Europe\",\n",
    "    \"Gradient descent optimization techniques\",\n",
    "    \"Cooking Italian pasta dishes at home\",\n",
    "    \"Transfer learning with pretrained models\",\n",
    "    \"Growing tomatoes in your backyard garden\"\n",
    "]\n",
    "\n",
    "print(\"üìö Document Database:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"{i}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, docs, top_k=3):\n",
    "    \"\"\"\n",
    "    Search documents by semantic similarity\n",
    "    \n",
    "    Args:\n",
    "        query: Search string\n",
    "        docs: List of documents\n",
    "        top_k: Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List of (doc, score) tuples\n",
    "    \"\"\"\n",
    "    query_emb = np.array(get_embedding(query)).reshape(1, -1)\n",
    "    \n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        doc_emb = np.array(get_embedding(doc)).reshape(1, -1)\n",
    "        score = cosine_similarity(query_emb, doc_emb)[0][0]\n",
    "        scores.append((doc, score))\n",
    "    \n",
    "    # Sort by score (highest first)\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Semantic Search Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"deep learning and AI\",\n",
    "    \"outdoor activities and nature\",\n",
    "    \"food and recipes\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*75}\")\n",
    "    print(f\"üîç SEARCH QUERY: '{query}'\")\n",
    "    print('='*75)\n",
    "    \n",
    "    results = semantic_search(query, documents, top_k=3)\n",
    "    \n",
    "    for rank, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n{rank}. [Score: {score:.4f}]\")\n",
    "        print(f\"   {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Example : Question Similarity (FAQ Matching)\n",
    "\n",
    "Useful for customer support chatbots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq_questions = [\n",
    "    \"How do I reset my password?\",\n",
    "    \"What are your shipping costs?\",\n",
    "    \"Can I return an item?\",\n",
    "    \"How long does delivery take?\",\n",
    "    \"What payment methods do you accept?\"\n",
    "]\n",
    "\n",
    "user_questions = [\n",
    "    \"I forgot my login credentials\",\n",
    "    \"How much does shipping cost?\",\n",
    "    \"I want to send back a product\"\n",
    "]\n",
    "\n",
    "print(\"FAQ Database:\\n\")\n",
    "for i, q in enumerate(faq_questions, 1):\n",
    "    print(f\"{i}. {q}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*75)\n",
    "\n",
    "for user_q in user_questions:\n",
    "    print(f\"\\nüë§ User asks: '{user_q}'\")\n",
    "    \n",
    "    # Find best matching FAQ\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    \n",
    "    for faq_q in faq_questions:\n",
    "        score = calculate_similarity(user_q, faq_q)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = faq_q\n",
    "    \n",
    "    print(f\"ü§ñ Best match: '{best_match}'\")\n",
    "    print(f\"   Confidence: {best_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
